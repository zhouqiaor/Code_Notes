{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T09:12:27.243684Z",
     "start_time": "2020-03-18T09:12:27.238901Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import datetime\n",
    "import sys\n",
    "import np as numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T09:12:29.191694Z",
     "start_time": "2020-03-18T09:12:28.990955Z"
    },
    "_cell_guid": "8dd5a78e-7ade-4fa3-93c0-b18be1214740",
    "_uuid": "a0fbf8ebb5f35a7d7fff6dbf61e697ed1df995bb"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T09:15:50.647692Z",
     "start_time": "2020-03-18T09:15:50.638223Z"
    }
   },
   "outputs": [],
   "source": [
    "def loadDataSet(file_name, label_existed_flag):\n",
    "    feats = []\n",
    "    labels = []\n",
    "    fr = open(file_name)\n",
    "    lines = fr.readlines()\n",
    "    for line in lines:\n",
    "        temp = []\n",
    "        allInfo = line.strip().split(',')\n",
    "        dims = len(allInfo)\n",
    "        if label_existed_flag == 1:\n",
    "            for index in range(dims-1):\n",
    "                temp.append(float(allInfo[index]))\n",
    "            feats.append(temp)\n",
    "            labels.append(float(allInfo[dims-1]))\n",
    "        else:\n",
    "            for index in range(dims):\n",
    "                temp.append(float(allInfo[index]))\n",
    "            feats.append(temp)\n",
    "    fr.close()\n",
    "    feats = np.array(feats)\n",
    "    labels = np.array(labels)\n",
    "    return feats, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T09:15:55.191806Z",
     "start_time": "2020-03-18T09:15:55.187269Z"
    }
   },
   "outputs": [],
   "source": [
    "train_file =  \"../data/train_data.txt\"\n",
    "test_file = \"../data/test_data.txt\"\n",
    "predict_file = \"../projects/student/result.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T09:16:00.367676Z",
     "start_time": "2020-03-18T09:15:56.819023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.09 s, sys: 179 ms, total: 3.27 s\n",
      "Wall time: 3.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, y_train = loadDataSet(train_file, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T09:17:24.407681Z",
     "start_time": "2020-03-18T09:17:23.552408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 782 ms, sys: 28 ms, total: 810 ms\n",
      "Wall time: 809 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test, _ = loadDataSet(test_file, 0)\n",
    "answer_file = \"../projects/student/answer.txt\"\n",
    "_, y_test = loadDataSet(answer_file, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_func(theta, x):\n",
    "    return float(1) / (1 + math.e**(-x.dot(theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_gradient(theta, x, y):\n",
    "    first_calc = logistic_func(theta, x) - np.squeeze(y)\n",
    "    final_calc = first_calc.T.dot(x)\n",
    "    return final_calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_func(theta, x, y):\n",
    "    log_func_v = logistic_func(theta,x)\n",
    "    y = np.squeeze(y)\n",
    "    step1 = y * np.log(log_func_v)\n",
    "    step2 = (1-y) * np.log(1 - log_func_v)\n",
    "    final = -step1 - step2\n",
    "    return np.mean(final)\n",
    "    #return np.power(label - preval, 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_desc(theta_values, X, y, lr=.001, converge_change=.001):\n",
    "    #normalize\n",
    "    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "    #setup cost iter\n",
    "    cost_iter = []\n",
    "    cost = cost_func(theta_values, X, y)\n",
    "    cost_iter.append([0, cost])\n",
    "    change_cost = 1\n",
    "    i = 1\n",
    "    while(change_cost > converge_change):\n",
    "        old_cost = cost\n",
    "        theta_values = theta_values - (lr * log_gradient(theta_values, X, y))\n",
    "        cost = cost_func(theta_values, X, y)\n",
    "        cost_iter.append([i, cost])\n",
    "        change_cost = old_cost - cost\n",
    "        i+=1\n",
    "    return theta_values, np.array(cost_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_values(theta, X, hard=True):\n",
    "    #normalize\n",
    "    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "    pred_prob = logistic_func(theta, X)\n",
    "    pred_value = np.where(pred_prob >= .5, 1, 0)\n",
    "    if hard:\n",
    "        return pred_value\n",
    "    return pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = X_train.shape[1]\n",
    "shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_flip = np.logical_not(y_train) #flip Setosa to be 1 and Versicolor to zero to be consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 1., 1.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True, False, False])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_flip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.sum(y_flip == y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas = np.zeros(shape)\n",
    "betas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_values, cost_iter = grad_desc(betas, X_train, y_flip)\n",
    "#print(fitted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.69314718],\n",
       "       [1.        , 0.53198428],\n",
       "       [2.        , 0.48347423],\n",
       "       [3.        , 0.48142636],\n",
       "       [4.        , 0.48232484]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predicted_y = pred_values(fitted_values, X_train)\n",
    "predicted_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.sum(y_flip == predicted_y)/np.size(X_train, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_y = pred_values(fitted_values, X_test)\n",
    "predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2165"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_test == predicted_y)/np.size(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Iteration')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cost_iter[:,0], cost_iter[:,1])\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "# sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 1000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalize data\n",
    "norm_X = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
    "norm_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myargs = (norm_X, y_flip)\n",
    "type(myargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas = np.zeros(norm_X.shape[1])\n",
    "betas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbfgs_fitted = fmin_l_bfgs_b(cost_func, x0=betas, args=myargs, fprime=log_gradient)\n",
    "# lbfgs_fitted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.631375"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbfgs_predicted = pred_values(lbfgs_fitted[0], norm_X, hard=True)\n",
    "sum(lbfgs_predicted == y_flip)/np.size(X_train, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "_epsilon = np.sqrt(np.finfo(float).eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def fmin_bfgs(f, x0, fprime=None, args=(), gtol=1e-5, norm=np.Inf,\n",
    "              epsilon=_epsilon, maxiter=None, full_output=0, disp=1,\n",
    "              retall=0, callback=None):\n",
    "    \"\"\"\n",
    "    Minimize a function using the BFGS algorithm.\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable f(x,*args)\n",
    "        Objective function to be minimized.\n",
    "    x0 : ndarray\n",
    "        Initial guess.\n",
    "    fprime : callable f'(x,*args), optional\n",
    "        Gradient of f.\n",
    "    args : tuple, optional\n",
    "        Extra arguments passed to f and fprime.\n",
    "    gtol : float, optional\n",
    "        Gradient norm must be less than gtol before successful termination.\n",
    "    norm : float, optional\n",
    "        Order of norm (Inf is max, -Inf is min)\n",
    "    epsilon : int or ndarray, optional\n",
    "        If fprime is approximated, use this value for the step size.\n",
    "    callback : callable, optional\n",
    "        An optional user-supplied function to call after each\n",
    "        iteration.  Called as callback(xk), where xk is the\n",
    "        current parameter vector.\n",
    "    maxiter : int, optional\n",
    "        Maximum number of iterations to perform.\n",
    "    full_output : bool, optional\n",
    "        If True,return fopt, func_calls, grad_calls, and warnflag\n",
    "        in addition to xopt.\n",
    "    disp : bool, optional\n",
    "        Print convergence message if True.\n",
    "    retall : bool, optional\n",
    "        Return a list of results at each iteration if True.\n",
    "    Returns\n",
    "    -------\n",
    "    xopt : ndarray\n",
    "        Parameters which minimize f, i.e. f(xopt) == fopt.\n",
    "    fopt : float\n",
    "        Minimum value.\n",
    "    gopt : ndarray\n",
    "        Value of gradient at minimum, f'(xopt), which should be near 0.\n",
    "    Bopt : ndarray\n",
    "        Value of 1/f''(xopt), i.e. the inverse hessian matrix.\n",
    "    func_calls : int\n",
    "        Number of function_calls made.\n",
    "    grad_calls : int\n",
    "        Number of gradient calls made.\n",
    "    warnflag : integer\n",
    "        1 : Maximum number of iterations exceeded.\n",
    "        2 : Gradient and/or function calls not changing.\n",
    "        3 : NaN result encountered.\n",
    "    allvecs  :  list\n",
    "        The value of xopt at each iteration.  Only returned if retall is True.\n",
    "    See also\n",
    "    --------\n",
    "    minimize: Interface to minimization algorithms for multivariate\n",
    "        functions. See the 'BFGS' `method` in particular.\n",
    "    Notes\n",
    "    -----\n",
    "    Optimize the function, f, whose gradient is given by fprime\n",
    "    using the quasi-Newton method of Broyden, Fletcher, Goldfarb,\n",
    "    and Shanno (BFGS)\n",
    "    References\n",
    "    ----------\n",
    "    Wright, and Nocedal 'Numerical Optimization', 1999, pg. 198.\n",
    "    \"\"\"\n",
    "    opts = {'gtol': gtol,\n",
    "            'norm': norm,\n",
    "            'eps': epsilon,\n",
    "            'disp': disp,\n",
    "            'maxiter': maxiter,\n",
    "            'return_all': retall}\n",
    "\n",
    "    res = _minimize_bfgs(f, x0, args, fprime, callback=callback, **opts)\n",
    "\n",
    "    if full_output:\n",
    "        retlist = (res['x'], res['fun'], res['jac'], res['hess_inv'],\n",
    "                   res['nfev'], res['njev'], res['status'])\n",
    "        if retall:\n",
    "            retlist += (res['allvecs'], )\n",
    "        return retlist\n",
    "    else:\n",
    "        if retall:\n",
    "            return res['x'], res['allvecs']\n",
    "        else:\n",
    "            return res['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def _minimize_bfgs(fun, x0, args=(), jac=None, callback=None,\n",
    "                   gtol=1e-5, norm=np.Inf, eps=_epsilon, maxiter=None,\n",
    "                   disp=False, return_all=False,\n",
    "                   **unknown_options):\n",
    "    \"\"\"\n",
    "    Minimization of scalar function of one or more variables using the\n",
    "    BFGS algorithm.\n",
    "    Options\n",
    "    -------\n",
    "    disp : bool\n",
    "        Set to True to print convergence messages.\n",
    "    maxiter : int\n",
    "        Maximum number of iterations to perform.\n",
    "    gtol : float\n",
    "        Gradient norm must be less than `gtol` before successful\n",
    "        termination.\n",
    "    norm : float\n",
    "        Order of norm (Inf is max, -Inf is min).\n",
    "    eps : float or ndarray\n",
    "        If `jac` is approximated, use this value for the step size.\n",
    "    \"\"\"\n",
    "    _check_unknown_options(unknown_options)\n",
    "    f = fun\n",
    "    fprime = jac\n",
    "    epsilon = eps\n",
    "    retall = return_all\n",
    "\n",
    "    x0 = np.asarray(x0).flatten()\n",
    "    if x0.ndim == 0:\n",
    "        x0.shape = (1,)\n",
    "    if maxiter is None:\n",
    "        maxiter = len(x0) * 200\n",
    "    func_calls, f = wrap_function(f, args)\n",
    "\n",
    "    old_fval = f(x0)\n",
    "\n",
    "    if fprime is None:\n",
    "        grad_calls, myfprime = wrap_function(approx_fprime, (f, epsilon))\n",
    "    else:\n",
    "        grad_calls, myfprime = wrap_function(fprime, args)\n",
    "    gfk = myfprime(x0)\n",
    "    k = 0\n",
    "    N = len(x0)\n",
    "    I = np.eye(N, dtype=int)\n",
    "    Hk = I\n",
    "\n",
    "    # Sets the initial step guess to dx ~ 1\n",
    "    old_old_fval = old_fval + np.linalg.norm(gfk) / 2\n",
    "\n",
    "    xk = x0\n",
    "    if retall:\n",
    "        allvecs = [x0]\n",
    "    warnflag = 0\n",
    "    gnorm = vecnorm(gfk, ord=norm)\n",
    "    while (gnorm > gtol) and (k < maxiter):\n",
    "        pk = -np.dot(Hk, gfk)\n",
    "        try:\n",
    "            alpha_k, fc, gc, old_fval, old_old_fval, gfkp1 = \\\n",
    "                     _line_search_wolfe12(f, myfprime, xk, pk, gfk,\n",
    "                                          old_fval, old_old_fval, amin=1e-100, amax=1e100)\n",
    "        except _LineSearchError:\n",
    "            # Line search failed to find a better solution.\n",
    "            warnflag = 2\n",
    "            break\n",
    "\n",
    "        xkp1 = xk + alpha_k * pk\n",
    "        if retall:\n",
    "            allvecs.append(xkp1)\n",
    "        sk = xkp1 - xk\n",
    "        xk = xkp1\n",
    "        if gfkp1 is None:\n",
    "            gfkp1 = myfprime(xkp1)\n",
    "\n",
    "        yk = gfkp1 - gfk\n",
    "        gfk = gfkp1\n",
    "        if callback is not None:\n",
    "            callback(xk)\n",
    "        k += 1\n",
    "        gnorm = vecnorm(gfk, ord=norm)\n",
    "        if (gnorm <= gtol):\n",
    "            break\n",
    "\n",
    "        if not np.isfinite(old_fval):\n",
    "            # We correctly found +-Inf as optimal value, or something went\n",
    "            # wrong.\n",
    "            warnflag = 2\n",
    "            break\n",
    "\n",
    "        try:  # this was handled in numeric, let it remaines for more safety\n",
    "            rhok = 1.0 / (np.dot(yk, sk))\n",
    "        except ZeroDivisionError:\n",
    "            rhok = 1000.0\n",
    "            if disp:\n",
    "                print(\"Divide-by-zero encountered: rhok assumed large\")\n",
    "        if isinf(rhok):  # this is patch for np\n",
    "            rhok = 1000.0\n",
    "            if disp:\n",
    "                print(\"Divide-by-zero encountered: rhok assumed large\")\n",
    "        A1 = I - sk[:, np.newaxis] * yk[np.newaxis, :] * rhok\n",
    "        A2 = I - yk[:, np.newaxis] * sk[np.newaxis, :] * rhok\n",
    "        Hk = np.dot(A1, np.dot(Hk, A2)) + (rhok * sk[:, np.newaxis] *\n",
    "                                                 sk[np.newaxis, :])\n",
    "\n",
    "    fval = old_fval\n",
    "\n",
    "    if warnflag == 2:\n",
    "        msg = _status_message['pr_loss']\n",
    "    elif k >= maxiter:\n",
    "        warnflag = 1\n",
    "        msg = _status_message['maxiter']\n",
    "    elif np.isnan(gnorm) or np.isnan(fval) or np.isnan(xk).any():\n",
    "        warnflag = 3\n",
    "        msg = _status_message['nan']\n",
    "    else:\n",
    "        msg = _status_message['success']\n",
    "\n",
    "    if disp:\n",
    "        print(\"%s%s\" % (\"Warning: \" if warnflag != 0 else \"\", msg))\n",
    "        print(\"         Current function value: %f\" % fval)\n",
    "        print(\"         Iterations: %d\" % k)\n",
    "        print(\"         Function evaluations: %d\" % func_calls[0])\n",
    "        print(\"         Gradient evaluations: %d\" % grad_calls[0])\n",
    "\n",
    "    result = OptimizeResult(fun=fval, jac=gfk, hess_inv=Hk, nfev=func_calls[0],\n",
    "                            njev=grad_calls[0], status=warnflag,\n",
    "                            success=(warnflag == 0), message=msg, x=xk,\n",
    "                            nit=k)\n",
    "    if retall:\n",
    "        result['allvecs'] = allvecs\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def _check_unknown_options(unknown_options):\n",
    "    if unknown_options:\n",
    "        msg = \", \".join(map(str, unknown_options.keys()))\n",
    "        # Stack level 4: this is called from _minimize_*, which is\n",
    "        # called from another function in SciPy. Level 4 is the first\n",
    "        # level in user code.\n",
    "        warnings.warn(\"Unknown solver options: %s\" % msg, OptimizeWarning, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def wrap_function(function, args):\n",
    "    ncalls = [0]\n",
    "    if function is None:\n",
    "        return ncalls, None\n",
    "\n",
    "    def function_wrapper(*wrapper_args):\n",
    "        ncalls[0] += 1\n",
    "        return function(*(wrapper_args + args))\n",
    "\n",
    "    return ncalls, function_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def vecnorm(x, ord=2):\n",
    "    if ord == np.Inf:\n",
    "        return np.amax(np.abs(x))\n",
    "    elif ord == -np.Inf:\n",
    "        return np.amin(np.abs(x))\n",
    "    else:\n",
    "        return np.sum(np.abs(x)**ord, axis=0)**(1.0 / ord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class _LineSearchError(RuntimeError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def _line_search_wolfe12(f, fprime, xk, pk, gfk, old_fval, old_old_fval,\n",
    "                         **kwargs):\n",
    "    \"\"\"\n",
    "    Same as line_search_wolfe1, but fall back to line_search_wolfe2 if\n",
    "    suitable step length is not found, and raise an exception if a\n",
    "    suitable step length is not found.\n",
    "    Raises\n",
    "    ------\n",
    "    _LineSearchError\n",
    "        If no suitable step size is found\n",
    "    \"\"\"\n",
    "\n",
    "    extra_condition = kwargs.pop('extra_condition', None)\n",
    "\n",
    "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
    "                             old_fval, old_old_fval,\n",
    "                             **kwargs)\n",
    "\n",
    "    if ret[0] is not None and extra_condition is not None:\n",
    "        xp1 = xk + ret[0] * pk\n",
    "        if not extra_condition(ret[0], xp1, ret[3], ret[5]):\n",
    "            # Reject step if extra_condition fails\n",
    "            ret = (None,)\n",
    "\n",
    "    if ret[0] is None:\n",
    "        # line search failed: try different one.\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore', LineSearchWarning)\n",
    "            kwargs2 = {}\n",
    "            for key in ('c1', 'c2', 'amax'):\n",
    "                if key in kwargs:\n",
    "                    kwargs2[key] = kwargs[key]\n",
    "            ret = line_search_wolfe2(f, fprime, xk, pk, gfk,\n",
    "                                     old_fval, old_old_fval,\n",
    "                                     extra_condition=extra_condition,\n",
    "                                     **kwargs2)\n",
    "\n",
    "    if ret[0] is None:\n",
    "        raise _LineSearchError()\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions\n",
    "---------\n",
    ".. autosummary::\n",
    "   :toctree: generated/\n",
    "\n",
    "    line_search_armijo\n",
    "    line_search_wolfe1\n",
    "    line_search_wolfe2\n",
    "    scalar_search_wolfe1\n",
    "    scalar_search_wolfe2\n",
    "\n",
    "\"\"\"\n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "from warnings import warn\n",
    "\n",
    "from scipy.optimize import minpack2\n",
    "import numpy as np\n",
    "from scipy._lib.six import xrange\n",
    "\n",
    "__all__ = ['LineSearchWarning', 'line_search_wolfe1', 'line_search_wolfe2',\n",
    "           'scalar_search_wolfe1', 'scalar_search_wolfe2',\n",
    "           'line_search_armijo']\n",
    "\n",
    "class LineSearchWarning(RuntimeWarning):\n",
    "    pass\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Minpack's Wolfe line and scalar searches\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def line_search_wolfe1(f, fprime, xk, pk, gfk=None,\n",
    "                       old_fval=None, old_old_fval=None,\n",
    "                       args=(), c1=1e-4, c2=0.9, amax=50, amin=1e-8,\n",
    "                       xtol=1e-14):\n",
    "    \"\"\"\n",
    "    As `scalar_search_wolfe1` but do a line search to direction `pk`\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        Function `f(x)`\n",
    "    fprime : callable\n",
    "        Gradient of `f`\n",
    "    xk : array_like\n",
    "        Current point\n",
    "    pk : array_like\n",
    "        Search direction\n",
    "\n",
    "    gfk : array_like, optional\n",
    "        Gradient of `f` at point `xk`\n",
    "    old_fval : float, optional\n",
    "        Value of `f` at point `xk`\n",
    "    old_old_fval : float, optional\n",
    "        Value of `f` at point preceding `xk`\n",
    "\n",
    "    The rest of the parameters are the same as for `scalar_search_wolfe1`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    stp, f_count, g_count, fval, old_fval\n",
    "        As in `line_search_wolfe1`\n",
    "    gval : array\n",
    "        Gradient of `f` at the final point\n",
    "\n",
    "    \"\"\"\n",
    "    if gfk is None:\n",
    "        gfk = fprime(xk)\n",
    "\n",
    "    if isinstance(fprime, tuple):\n",
    "        eps = fprime[1]\n",
    "        fprime = fprime[0]\n",
    "        newargs = (f, eps) + args\n",
    "        gradient = False\n",
    "    else:\n",
    "        newargs = args\n",
    "        gradient = True\n",
    "\n",
    "    gval = [gfk]\n",
    "    gc = [0]\n",
    "    fc = [0]\n",
    "\n",
    "    def phi(s):\n",
    "        fc[0] += 1\n",
    "        return f(xk + s*pk, *args)\n",
    "\n",
    "    def derphi(s):\n",
    "        gval[0] = fprime(xk + s*pk, *newargs)\n",
    "        if gradient:\n",
    "            gc[0] += 1\n",
    "        else:\n",
    "            fc[0] += len(xk) + 1\n",
    "        return np.dot(gval[0], pk)\n",
    "\n",
    "    derphi0 = np.dot(gfk, pk)\n",
    "\n",
    "    stp, fval, old_fval = scalar_search_wolfe1(\n",
    "            phi, derphi, old_fval, old_old_fval, derphi0,\n",
    "            c1=c1, c2=c2, amax=amax, amin=amin, xtol=xtol)\n",
    "\n",
    "    return stp, fc[0], gc[0], fval, old_fval, gval[0]\n",
    "\n",
    "\n",
    "def scalar_search_wolfe1(phi, derphi, phi0=None, old_phi0=None, derphi0=None,\n",
    "                         c1=1e-4, c2=0.9,\n",
    "                         amax=50, amin=1e-8, xtol=1e-14):\n",
    "    \"\"\"\n",
    "    Scalar function search for alpha that satisfies strong Wolfe conditions\n",
    "\n",
    "    alpha > 0 is assumed to be a descent direction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    phi : callable phi(alpha)\n",
    "        Function at point `alpha`\n",
    "    derphi : callable phi'(alpha)\n",
    "        Objective function derivative. Returns a scalar.\n",
    "    phi0 : float, optional\n",
    "        Value of phi at 0\n",
    "    old_phi0 : float, optional\n",
    "        Value of phi at previous point\n",
    "    derphi0 : float, optional\n",
    "        Value derphi at 0\n",
    "    c1 : float, optional\n",
    "        Parameter for Armijo condition rule.\n",
    "    c2 : float, optional\n",
    "        Parameter for curvature condition rule.\n",
    "    amax, amin : float, optional\n",
    "        Maximum and minimum step size\n",
    "    xtol : float, optional\n",
    "        Relative tolerance for an acceptable step.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    alpha : float\n",
    "        Step size, or None if no suitable step was found\n",
    "    phi : float\n",
    "        Value of `phi` at the new point `alpha`\n",
    "    phi0 : float\n",
    "        Value of `phi` at `alpha=0`\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Uses routine DCSRCH from MINPACK.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if phi0 is None:\n",
    "        phi0 = phi(0.)\n",
    "    if derphi0 is None:\n",
    "        derphi0 = derphi(0.)\n",
    "\n",
    "    if old_phi0 is not None and derphi0 != 0:\n",
    "        alpha1 = min(1.0, 1.01*2*(phi0 - old_phi0)/derphi0)\n",
    "        if alpha1 < 0:\n",
    "            alpha1 = 1.0\n",
    "    else:\n",
    "        alpha1 = 1.0\n",
    "\n",
    "    phi1 = phi0\n",
    "    derphi1 = derphi0\n",
    "    isave = np.zeros((2,), np.intc)\n",
    "    dsave = np.zeros((13,), float)\n",
    "    task = b'START'\n",
    "\n",
    "    maxiter = 100\n",
    "    for i in xrange(maxiter):\n",
    "        stp, phi1, derphi1, task = minpack2.dcsrch(alpha1, phi1, derphi1,\n",
    "                                                   c1, c2, xtol, task,\n",
    "                                                   amin, amax, isave, dsave)\n",
    "        if task[:2] == b'FG':\n",
    "            alpha1 = stp\n",
    "            phi1 = phi(stp)\n",
    "            derphi1 = derphi(stp)\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        # maxiter reached, the line search did not converge\n",
    "        stp = None\n",
    "\n",
    "    if task[:5] == b'ERROR' or task[:4] == b'WARN':\n",
    "        stp = None  # failed\n",
    "\n",
    "    return stp, phi1, phi0\n",
    "\n",
    "\n",
    "line_search = line_search_wolfe1\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Pure-Python Wolfe line and scalar searches\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def line_search_wolfe2(f, myfprime, xk, pk, gfk=None, old_fval=None,\n",
    "                       old_old_fval=None, args=(), c1=1e-4, c2=0.9, amax=None,\n",
    "                       extra_condition=None, maxiter=10):\n",
    "    \"\"\"Find alpha that satisfies strong Wolfe conditions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable f(x,*args)\n",
    "        Objective function.\n",
    "    myfprime : callable f'(x,*args)\n",
    "        Objective function gradient.\n",
    "    xk : ndarray\n",
    "        Starting point.\n",
    "    pk : ndarray\n",
    "        Search direction.\n",
    "    gfk : ndarray, optional\n",
    "        Gradient value for x=xk (xk being the current parameter\n",
    "        estimate). Will be recomputed if omitted.\n",
    "    old_fval : float, optional\n",
    "        Function value for x=xk. Will be recomputed if omitted.\n",
    "    old_old_fval : float, optional\n",
    "        Function value for the point preceding x=xk\n",
    "    args : tuple, optional\n",
    "        Additional arguments passed to objective function.\n",
    "    c1 : float, optional\n",
    "        Parameter for Armijo condition rule.\n",
    "    c2 : float, optional\n",
    "        Parameter for curvature condition rule.\n",
    "    amax : float, optional\n",
    "        Maximum step size\n",
    "    extra_condition : callable, optional\n",
    "        A callable of the form ``extra_condition(alpha, x, f, g)``\n",
    "        returning a boolean. Arguments are the proposed step ``alpha``\n",
    "        and the corresponding ``x``, ``f`` and ``g`` values. The line search \n",
    "        accepts the value of ``alpha`` only if this \n",
    "        callable returns ``True``. If the callable returns ``False`` \n",
    "        for the step length, the algorithm will continue with \n",
    "        new iterates. The callable is only called for iterates \n",
    "        satisfying the strong Wolfe conditions.\n",
    "    maxiter : int, optional\n",
    "        Maximum number of iterations to perform\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    alpha : float or None\n",
    "        Alpha for which ``x_new = x0 + alpha * pk``,\n",
    "        or None if the line search algorithm did not converge.\n",
    "    fc : int\n",
    "        Number of function evaluations made.\n",
    "    gc : int\n",
    "        Number of gradient evaluations made.\n",
    "    new_fval : float or None\n",
    "        New function value ``f(x_new)=f(x0+alpha*pk)``,\n",
    "        or None if the line search algorithm did not converge.\n",
    "    old_fval : float\n",
    "        Old function value ``f(x0)``.\n",
    "    new_slope : float or None\n",
    "        The local slope along the search direction at the\n",
    "        new value ``<myfprime(x_new), pk>``,\n",
    "        or None if the line search algorithm did not converge.\n",
    "\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Uses the line search algorithm to enforce strong Wolfe\n",
    "    conditions.  See Wright and Nocedal, 'Numerical Optimization',\n",
    "    1999, pg. 59-60.\n",
    "\n",
    "    For the zoom phase it uses an algorithm by [...].\n",
    "\n",
    "    \"\"\"\n",
    "    fc = [0]\n",
    "    gc = [0]\n",
    "    gval = [None]\n",
    "    gval_alpha = [None]\n",
    "\n",
    "    def phi(alpha):\n",
    "        fc[0] += 1\n",
    "        return f(xk + alpha * pk, *args)\n",
    "\n",
    "    if isinstance(myfprime, tuple):\n",
    "        def derphi(alpha):\n",
    "            fc[0] += len(xk) + 1\n",
    "            eps = myfprime[1]\n",
    "            fprime = myfprime[0]\n",
    "            newargs = (f, eps) + args\n",
    "            gval[0] = fprime(xk + alpha * pk, *newargs)  # store for later use\n",
    "            gval_alpha[0] = alpha\n",
    "            return np.dot(gval[0], pk)\n",
    "    else:\n",
    "        fprime = myfprime\n",
    "\n",
    "        def derphi(alpha):\n",
    "            gc[0] += 1\n",
    "            gval[0] = fprime(xk + alpha * pk, *args)  # store for later use\n",
    "            gval_alpha[0] = alpha\n",
    "            return np.dot(gval[0], pk)\n",
    "\n",
    "    if gfk is None:\n",
    "        gfk = fprime(xk, *args)\n",
    "    derphi0 = np.dot(gfk, pk)\n",
    "\n",
    "    if extra_condition is not None:\n",
    "        # Add the current gradient as argument, to avoid needless\n",
    "        # re-evaluation\n",
    "        def extra_condition2(alpha, phi):\n",
    "            if gval_alpha[0] != alpha:\n",
    "                derphi(alpha)\n",
    "            x = xk + alpha * pk\n",
    "            return extra_condition(alpha, x, phi, gval[0])\n",
    "    else:\n",
    "        extra_condition2 = None\n",
    "\n",
    "    alpha_star, phi_star, old_fval, derphi_star = scalar_search_wolfe2(\n",
    "            phi, derphi, old_fval, old_old_fval, derphi0, c1, c2, amax,\n",
    "            extra_condition2, maxiter=maxiter)\n",
    "\n",
    "    if derphi_star is None:\n",
    "        warn('The line search algorithm did not converge', LineSearchWarning)\n",
    "    else:\n",
    "        # derphi_star is a number (derphi) -- so use the most recently\n",
    "        # calculated gradient used in computing it derphi = gfk*pk\n",
    "        # this is the gradient at the next step no need to compute it\n",
    "        # again in the outer loop.\n",
    "        derphi_star = gval[0]\n",
    "\n",
    "    return alpha_star, fc[0], gc[0], phi_star, old_fval, derphi_star\n",
    "\n",
    "\n",
    "def scalar_search_wolfe2(phi, derphi, phi0=None,\n",
    "                         old_phi0=None, derphi0=None,\n",
    "                         c1=1e-4, c2=0.9, amax=None,\n",
    "                         extra_condition=None, maxiter=10):\n",
    "    \"\"\"Find alpha that satisfies strong Wolfe conditions.\n",
    "\n",
    "    alpha > 0 is assumed to be a descent direction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    phi : callable phi(alpha)\n",
    "        Objective scalar function.\n",
    "    derphi : callable phi'(alpha)\n",
    "        Objective function derivative. Returns a scalar.\n",
    "    phi0 : float, optional\n",
    "        Value of phi at 0\n",
    "    old_phi0 : float, optional\n",
    "        Value of phi at previous point\n",
    "    derphi0 : float, optional\n",
    "        Value of derphi at 0\n",
    "    c1 : float, optional\n",
    "        Parameter for Armijo condition rule.\n",
    "    c2 : float, optional\n",
    "        Parameter for curvature condition rule.\n",
    "    amax : float, optional\n",
    "        Maximum step size\n",
    "    extra_condition : callable, optional\n",
    "        A callable of the form ``extra_condition(alpha, phi_value)``\n",
    "        returning a boolean. The line search accepts the value\n",
    "        of ``alpha`` only if this callable returns ``True``.\n",
    "        If the callable returns ``False`` for the step length,\n",
    "        the algorithm will continue with new iterates.\n",
    "        The callable is only called for iterates satisfying\n",
    "        the strong Wolfe conditions.\n",
    "    maxiter : int, optional\n",
    "        Maximum number of iterations to perform\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    alpha_star : float or None\n",
    "        Best alpha, or None if the line search algorithm did not converge.\n",
    "    phi_star : float\n",
    "        phi at alpha_star\n",
    "    phi0 : float\n",
    "        phi at 0\n",
    "    derphi_star : float or None\n",
    "        derphi at alpha_star, or None if the line search algorithm\n",
    "        did not converge.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Uses the line search algorithm to enforce strong Wolfe\n",
    "    conditions.  See Wright and Nocedal, 'Numerical Optimization',\n",
    "    1999, pg. 59-60.\n",
    "\n",
    "    For the zoom phase it uses an algorithm by [...].\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if phi0 is None:\n",
    "        phi0 = phi(0.)\n",
    "\n",
    "    if derphi0 is None:\n",
    "        derphi0 = derphi(0.)\n",
    "\n",
    "    alpha0 = 0\n",
    "    if old_phi0 is not None and derphi0 != 0:\n",
    "        alpha1 = min(1.0, 1.01*2*(phi0 - old_phi0)/derphi0)\n",
    "    else:\n",
    "        alpha1 = 1.0\n",
    "\n",
    "    if alpha1 < 0:\n",
    "        alpha1 = 1.0\n",
    "\n",
    "    if amax is not None:\n",
    "        alpha1 = min(alpha1, amax)\n",
    "\n",
    "    phi_a1 = phi(alpha1)\n",
    "    #derphi_a1 = derphi(alpha1)  evaluated below\n",
    "\n",
    "    phi_a0 = phi0\n",
    "    derphi_a0 = derphi0\n",
    "\n",
    "    if extra_condition is None:\n",
    "        extra_condition = lambda alpha, phi: True\n",
    "\n",
    "    for i in xrange(maxiter):\n",
    "        if alpha1 == 0 or (amax is not None and alpha0 == amax):\n",
    "            # alpha1 == 0: This shouldn't happen. Perhaps the increment has\n",
    "            # slipped below machine precision?\n",
    "            alpha_star = None\n",
    "            phi_star = phi0\n",
    "            phi0 = old_phi0\n",
    "            derphi_star = None\n",
    "\n",
    "            if alpha1 == 0:\n",
    "                msg = 'Rounding errors prevent the line search from converging'\n",
    "            else:\n",
    "                msg = \"The line search algorithm could not find a solution \" + \\\n",
    "                      \"less than or equal to amax: %s\" % amax\n",
    "\n",
    "            warn(msg, LineSearchWarning)\n",
    "            break\n",
    "\n",
    "        if (phi_a1 > phi0 + c1 * alpha1 * derphi0) or \\\n",
    "           ((phi_a1 >= phi_a0) and (i > 1)):\n",
    "            alpha_star, phi_star, derphi_star = \\\n",
    "                        _zoom(alpha0, alpha1, phi_a0,\n",
    "                              phi_a1, derphi_a0, phi, derphi,\n",
    "                              phi0, derphi0, c1, c2, extra_condition)\n",
    "            break\n",
    "\n",
    "        derphi_a1 = derphi(alpha1)\n",
    "        if (abs(derphi_a1) <= -c2*derphi0):\n",
    "            if extra_condition(alpha1, phi_a1):\n",
    "                alpha_star = alpha1\n",
    "                phi_star = phi_a1\n",
    "                derphi_star = derphi_a1\n",
    "                break\n",
    "\n",
    "        if (derphi_a1 >= 0):\n",
    "            alpha_star, phi_star, derphi_star = \\\n",
    "                        _zoom(alpha1, alpha0, phi_a1,\n",
    "                              phi_a0, derphi_a1, phi, derphi,\n",
    "                              phi0, derphi0, c1, c2, extra_condition)\n",
    "            break\n",
    "\n",
    "        alpha2 = 2 * alpha1  # increase by factor of two on each iteration\n",
    "        if amax is not None:\n",
    "            alpha2 = min(alpha2, amax)\n",
    "        alpha0 = alpha1\n",
    "        alpha1 = alpha2\n",
    "        phi_a0 = phi_a1\n",
    "        phi_a1 = phi(alpha1)\n",
    "        derphi_a0 = derphi_a1\n",
    "\n",
    "    else:\n",
    "        # stopping test maxiter reached\n",
    "        alpha_star = alpha1\n",
    "        phi_star = phi_a1\n",
    "        derphi_star = None\n",
    "        warn('The line search algorithm did not converge', LineSearchWarning)\n",
    "\n",
    "    return alpha_star, phi_star, phi0, derphi_star\n",
    "\n",
    "\n",
    "def _cubicmin(a, fa, fpa, b, fb, c, fc):\n",
    "    \"\"\"\n",
    "    Finds the minimizer for a cubic polynomial that goes through the\n",
    "    points (a,fa), (b,fb), and (c,fc) with derivative at a of fpa.\n",
    "\n",
    "    If no minimizer can be found return None\n",
    "\n",
    "    \"\"\"\n",
    "    # f(x) = A *(x-a)^3 + B*(x-a)^2 + C*(x-a) + D\n",
    "\n",
    "    with np.errstate(divide='raise', over='raise', invalid='raise'):\n",
    "        try:\n",
    "            C = fpa\n",
    "            db = b - a\n",
    "            dc = c - a\n",
    "            denom = (db * dc) ** 2 * (db - dc)\n",
    "            d1 = np.empty((2, 2))\n",
    "            d1[0, 0] = dc ** 2\n",
    "            d1[0, 1] = -db ** 2\n",
    "            d1[1, 0] = -dc ** 3\n",
    "            d1[1, 1] = db ** 3\n",
    "            [A, B] = np.dot(d1, np.asarray([fb - fa - C * db,\n",
    "                                            fc - fa - C * dc]).flatten())\n",
    "            A /= denom\n",
    "            B /= denom\n",
    "            radical = B * B - 3 * A * C\n",
    "            xmin = a + (-B + np.sqrt(radical)) / (3 * A)\n",
    "        except ArithmeticError:\n",
    "            return None\n",
    "    if not np.isfinite(xmin):\n",
    "        return None\n",
    "    return xmin\n",
    "\n",
    "\n",
    "def _quadmin(a, fa, fpa, b, fb):\n",
    "    \"\"\"\n",
    "    Finds the minimizer for a quadratic polynomial that goes through\n",
    "    the points (a,fa), (b,fb) with derivative at a of fpa,\n",
    "\n",
    "    \"\"\"\n",
    "    # f(x) = B*(x-a)^2 + C*(x-a) + D\n",
    "    with np.errstate(divide='raise', over='raise', invalid='raise'):\n",
    "        try:\n",
    "            D = fa\n",
    "            C = fpa\n",
    "            db = b - a * 1.0\n",
    "            B = (fb - D - C * db) / (db * db)\n",
    "            xmin = a - C / (2.0 * B)\n",
    "        except ArithmeticError:\n",
    "            return None\n",
    "    if not np.isfinite(xmin):\n",
    "        return None\n",
    "    return xmin\n",
    "\n",
    "\n",
    "def _zoom(a_lo, a_hi, phi_lo, phi_hi, derphi_lo,\n",
    "          phi, derphi, phi0, derphi0, c1, c2, extra_condition):\n",
    "    \"\"\"\n",
    "    Part of the optimization algorithm in `scalar_search_wolfe2`.\n",
    "    \"\"\"\n",
    "\n",
    "    maxiter = 10\n",
    "    i = 0\n",
    "    delta1 = 0.2  # cubic interpolant check\n",
    "    delta2 = 0.1  # quadratic interpolant check\n",
    "    phi_rec = phi0\n",
    "    a_rec = 0\n",
    "    while True:\n",
    "        # interpolate to find a trial step length between a_lo and\n",
    "        # a_hi Need to choose interpolation here.  Use cubic\n",
    "        # interpolation and then if the result is within delta *\n",
    "        # dalpha or outside of the interval bounded by a_lo or a_hi\n",
    "        # then use quadratic interpolation, if the result is still too\n",
    "        # close, then use bisection\n",
    "\n",
    "        dalpha = a_hi - a_lo\n",
    "        if dalpha < 0:\n",
    "            a, b = a_hi, a_lo\n",
    "        else:\n",
    "            a, b = a_lo, a_hi\n",
    "\n",
    "        # minimizer of cubic interpolant\n",
    "        # (uses phi_lo, derphi_lo, phi_hi, and the most recent value of phi)\n",
    "        #\n",
    "        # if the result is too close to the end points (or out of the\n",
    "        # interval) then use quadratic interpolation with phi_lo,\n",
    "        # derphi_lo and phi_hi if the result is still too close to the\n",
    "        # end points (or out of the interval) then use bisection\n",
    "\n",
    "        if (i > 0):\n",
    "            cchk = delta1 * dalpha\n",
    "            a_j = _cubicmin(a_lo, phi_lo, derphi_lo, a_hi, phi_hi,\n",
    "                            a_rec, phi_rec)\n",
    "        if (i == 0) or (a_j is None) or (a_j > b - cchk) or (a_j < a + cchk):\n",
    "            qchk = delta2 * dalpha\n",
    "            a_j = _quadmin(a_lo, phi_lo, derphi_lo, a_hi, phi_hi)\n",
    "            if (a_j is None) or (a_j > b-qchk) or (a_j < a+qchk):\n",
    "                a_j = a_lo + 0.5*dalpha\n",
    "\n",
    "        # Check new value of a_j\n",
    "\n",
    "        phi_aj = phi(a_j)\n",
    "        if (phi_aj > phi0 + c1*a_j*derphi0) or (phi_aj >= phi_lo):\n",
    "            phi_rec = phi_hi\n",
    "            a_rec = a_hi\n",
    "            a_hi = a_j\n",
    "            phi_hi = phi_aj\n",
    "        else:\n",
    "            derphi_aj = derphi(a_j)\n",
    "            if abs(derphi_aj) <= -c2*derphi0 and extra_condition(a_j, phi_aj):\n",
    "                a_star = a_j\n",
    "                val_star = phi_aj\n",
    "                valprime_star = derphi_aj\n",
    "                break\n",
    "            if derphi_aj*(a_hi - a_lo) >= 0:\n",
    "                phi_rec = phi_hi\n",
    "                a_rec = a_hi\n",
    "                a_hi = a_lo\n",
    "                phi_hi = phi_lo\n",
    "            else:\n",
    "                phi_rec = phi_lo\n",
    "                a_rec = a_lo\n",
    "            a_lo = a_j\n",
    "            phi_lo = phi_aj\n",
    "            derphi_lo = derphi_aj\n",
    "        i += 1\n",
    "        if (i > maxiter):\n",
    "            # Failed to find a conforming step size\n",
    "            a_star = None\n",
    "            val_star = None\n",
    "            valprime_star = None\n",
    "            break\n",
    "    return a_star, val_star, valprime_star\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Armijo line and scalar searches\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def line_search_armijo(f, xk, pk, gfk, old_fval, args=(), c1=1e-4, alpha0=1):\n",
    "    \"\"\"Minimize over alpha, the function ``f(xk+alpha pk)``.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        Function to be minimized.\n",
    "    xk : array_like\n",
    "        Current point.\n",
    "    pk : array_like\n",
    "        Search direction.\n",
    "    gfk : array_like\n",
    "        Gradient of `f` at point `xk`.\n",
    "    old_fval : float\n",
    "        Value of `f` at point `xk`.\n",
    "    args : tuple, optional\n",
    "        Optional arguments.\n",
    "    c1 : float, optional\n",
    "        Value to control stopping criterion.\n",
    "    alpha0 : scalar, optional\n",
    "        Value of `alpha` at start of the optimization.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    alpha\n",
    "    f_count\n",
    "    f_val_at_alpha\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Uses the interpolation algorithm (Armijo backtracking) as suggested by\n",
    "    Wright and Nocedal in 'Numerical Optimization', 1999, pg. 56-57\n",
    "\n",
    "    \"\"\"\n",
    "    xk = np.atleast_1d(xk)\n",
    "    fc = [0]\n",
    "\n",
    "    def phi(alpha1):\n",
    "        fc[0] += 1\n",
    "        return f(xk + alpha1*pk, *args)\n",
    "\n",
    "    if old_fval is None:\n",
    "        phi0 = phi(0.)\n",
    "    else:\n",
    "        phi0 = old_fval  # compute f(xk) -- done in past loop\n",
    "\n",
    "    derphi0 = np.dot(gfk, pk)\n",
    "    alpha, phi1 = scalar_search_armijo(phi, phi0, derphi0, c1=c1,\n",
    "                                       alpha0=alpha0)\n",
    "    return alpha, fc[0], phi1\n",
    "\n",
    "\n",
    "def line_search_BFGS(f, xk, pk, gfk, old_fval, args=(), c1=1e-4, alpha0=1):\n",
    "    \"\"\"\n",
    "    Compatibility wrapper for `line_search_armijo`\n",
    "    \"\"\"\n",
    "    r = line_search_armijo(f, xk, pk, gfk, old_fval, args=args, c1=c1,\n",
    "                           alpha0=alpha0)\n",
    "    return r[0], r[1], 0, r[2]\n",
    "\n",
    "\n",
    "def scalar_search_armijo(phi, phi0, derphi0, c1=1e-4, alpha0=1, amin=0):\n",
    "    \"\"\"Minimize over alpha, the function ``phi(alpha)``.\n",
    "\n",
    "    Uses the interpolation algorithm (Armijo backtracking) as suggested by\n",
    "    Wright and Nocedal in 'Numerical Optimization', 1999, pg. 56-57\n",
    "\n",
    "    alpha > 0 is assumed to be a descent direction.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    alpha\n",
    "    phi1\n",
    "\n",
    "    \"\"\"\n",
    "    phi_a0 = phi(alpha0)\n",
    "    if phi_a0 <= phi0 + c1*alpha0*derphi0:\n",
    "        return alpha0, phi_a0\n",
    "\n",
    "    # Otherwise compute the minimizer of a quadratic interpolant:\n",
    "\n",
    "    alpha1 = -(derphi0) * alpha0**2 / 2.0 / (phi_a0 - phi0 - derphi0 * alpha0)\n",
    "    phi_a1 = phi(alpha1)\n",
    "\n",
    "    if (phi_a1 <= phi0 + c1*alpha1*derphi0):\n",
    "        return alpha1, phi_a1\n",
    "\n",
    "    # Otherwise loop with cubic interpolation until we find an alpha which\n",
    "    # satisfies the first Wolfe condition (since we are backtracking, we will\n",
    "    # assume that the value of alpha is not too small and satisfies the second\n",
    "    # condition.\n",
    "\n",
    "    while alpha1 > amin:       # we are assuming alpha>0 is a descent direction\n",
    "        factor = alpha0**2 * alpha1**2 * (alpha1-alpha0)\n",
    "        a = alpha0**2 * (phi_a1 - phi0 - derphi0*alpha1) - \\\n",
    "            alpha1**2 * (phi_a0 - phi0 - derphi0*alpha0)\n",
    "        a = a / factor\n",
    "        b = -alpha0**3 * (phi_a1 - phi0 - derphi0*alpha1) + \\\n",
    "            alpha1**3 * (phi_a0 - phi0 - derphi0*alpha0)\n",
    "        b = b / factor\n",
    "\n",
    "        alpha2 = (-b + np.sqrt(abs(b**2 - 3 * a * derphi0))) / (3.0*a)\n",
    "        phi_a2 = phi(alpha2)\n",
    "\n",
    "        if (phi_a2 <= phi0 + c1*alpha2*derphi0):\n",
    "            return alpha2, phi_a2\n",
    "\n",
    "        if (alpha1 - alpha2) > alpha1 / 2.0 or (1 - alpha2/alpha1) < 0.96:\n",
    "            alpha2 = alpha1 / 2.0\n",
    "\n",
    "        alpha0 = alpha1\n",
    "        alpha1 = alpha2\n",
    "        phi_a0 = phi_a1\n",
    "        phi_a1 = phi_a2\n",
    "\n",
    "    # Failed to find a suitable step length\n",
    "    return None, phi_a1\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Non-monotone line search for DF-SANE\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def _nonmonotone_line_search_cruz(f, x_k, d, prev_fs, eta,\n",
    "                                  gamma=1e-4, tau_min=0.1, tau_max=0.5):\n",
    "    \"\"\"\n",
    "    Nonmonotone backtracking line search as described in [1]_\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        Function returning a tuple ``(f, F)`` where ``f`` is the value\n",
    "        of a merit function and ``F`` the residual.\n",
    "    x_k : ndarray\n",
    "        Initial position\n",
    "    d : ndarray\n",
    "        Search direction\n",
    "    prev_fs : float\n",
    "        List of previous merit function values. Should have ``len(prev_fs) <= M``\n",
    "        where ``M`` is the nonmonotonicity window parameter.\n",
    "    eta : float\n",
    "        Allowed merit function increase, see [1]_\n",
    "    gamma, tau_min, tau_max : float, optional\n",
    "        Search parameters, see [1]_\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    alpha : float\n",
    "        Step length\n",
    "    xp : ndarray\n",
    "        Next position\n",
    "    fp : float\n",
    "        Merit function value at next position\n",
    "    Fp : ndarray\n",
    "        Residual at next position\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    [1] \"Spectral residual method without gradient information for solving\n",
    "        large-scale nonlinear systems of equations.\" W. La Cruz,\n",
    "        J.M. Martinez, M. Raydan. Math. Comp. **75**, 1429 (2006).\n",
    "\n",
    "    \"\"\"\n",
    "    f_k = prev_fs[-1]\n",
    "    f_bar = max(prev_fs)\n",
    "\n",
    "    alpha_p = 1\n",
    "    alpha_m = 1\n",
    "    alpha = 1\n",
    "\n",
    "    while True:\n",
    "        xp = x_k + alpha_p * d\n",
    "        fp, Fp = f(xp)\n",
    "\n",
    "        if fp <= f_bar + eta - gamma * alpha_p**2 * f_k:\n",
    "            alpha = alpha_p\n",
    "            break\n",
    "\n",
    "        alpha_tp = alpha_p**2 * f_k / (fp + (2*alpha_p - 1)*f_k)\n",
    "\n",
    "        xp = x_k - alpha_m * d\n",
    "        fp, Fp = f(xp)\n",
    "\n",
    "        if fp <= f_bar + eta - gamma * alpha_m**2 * f_k:\n",
    "            alpha = -alpha_m\n",
    "            break\n",
    "\n",
    "        alpha_tm = alpha_m**2 * f_k / (fp + (2*alpha_m - 1)*f_k)\n",
    "\n",
    "        alpha_p = np.clip(alpha_tp, tau_min * alpha_p, tau_max * alpha_p)\n",
    "        alpha_m = np.clip(alpha_tm, tau_min * alpha_m, tau_max * alpha_m)\n",
    "\n",
    "    return alpha, xp, fp, Fp\n",
    "\n",
    "\n",
    "def _nonmonotone_line_search_cheng(f, x_k, d, f_k, C, Q, eta,\n",
    "                                   gamma=1e-4, tau_min=0.1, tau_max=0.5,\n",
    "                                   nu=0.85):\n",
    "    \"\"\"\n",
    "    Nonmonotone line search from [1]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        Function returning a tuple ``(f, F)`` where ``f`` is the value\n",
    "        of a merit function and ``F`` the residual.\n",
    "    x_k : ndarray\n",
    "        Initial position\n",
    "    d : ndarray\n",
    "        Search direction\n",
    "    f_k : float\n",
    "        Initial merit function value\n",
    "    C, Q : float\n",
    "        Control parameters. On the first iteration, give values\n",
    "        Q=1.0, C=f_k\n",
    "    eta : float\n",
    "        Allowed merit function increase, see [1]_\n",
    "    nu, gamma, tau_min, tau_max : float, optional\n",
    "        Search parameters, see [1]_\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    alpha : float\n",
    "        Step length\n",
    "    xp : ndarray\n",
    "        Next position\n",
    "    fp : float\n",
    "        Merit function value at next position\n",
    "    Fp : ndarray\n",
    "        Residual at next position\n",
    "    C : float\n",
    "        New value for the control parameter C\n",
    "    Q : float\n",
    "        New value for the control parameter Q\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] W. Cheng & D.-H. Li, ''A derivative-free nonmonotone line\n",
    "           search and its application to the spectral residual\n",
    "           method'', IMA J. Numer. Anal. 29, 814 (2009).\n",
    "\n",
    "    \"\"\"\n",
    "    alpha_p = 1\n",
    "    alpha_m = 1\n",
    "    alpha = 1\n",
    "\n",
    "    while True:\n",
    "        xp = x_k + alpha_p * d\n",
    "        fp, Fp = f(xp)\n",
    "\n",
    "        if fp <= C + eta - gamma * alpha_p**2 * f_k:\n",
    "            alpha = alpha_p\n",
    "            break\n",
    "\n",
    "        alpha_tp = alpha_p**2 * f_k / (fp + (2*alpha_p - 1)*f_k)\n",
    "\n",
    "        xp = x_k - alpha_m * d\n",
    "        fp, Fp = f(xp)\n",
    "\n",
    "        if fp <= C + eta - gamma * alpha_m**2 * f_k:\n",
    "            alpha = -alpha_m\n",
    "            break\n",
    "\n",
    "        alpha_tm = alpha_m**2 * f_k / (fp + (2*alpha_m - 1)*f_k)\n",
    "\n",
    "        alpha_p = np.clip(alpha_tp, tau_min * alpha_p, tau_max * alpha_p)\n",
    "        alpha_m = np.clip(alpha_tm, tau_min * alpha_m, tau_max * alpha_m)\n",
    "\n",
    "    # Update C and Q\n",
    "    Q_next = nu * Q + 1\n",
    "    C = (nu * Q * (C + eta) + fp) / Q_next\n",
    "    Q = Q_next\n",
    "\n",
    "    return alpha, xp, fp, Fp, C, Q\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# standard status messages of optimizers\n",
    "_status_message = {'success': 'Optimization terminated successfully.',\n",
    "                   'maxfev': 'Maximum number of function evaluations has '\n",
    "                              'been exceeded.',\n",
    "                   'maxiter': 'Maximum number of iterations has been '\n",
    "                              'exceeded.',\n",
    "                   'pr_loss': 'Desired error not necessarily achieved due '\n",
    "                              'to precision loss.',\n",
    "                   'nan': 'NaN result encountered.'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from numpy import (atleast_1d, eye, mgrid, argmin, zeros, shape, squeeze,\n",
    "                   asarray, sqrt, Inf, asfarray, isinf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lbfgs_fitted2 = fmin_bfgs(cost_func, x0=betas, fprime=log_gradient, args=myargs, gtol=1e-5, norm=np.Inf,\n",
    "              epsilon=_epsilon, maxiter=None, full_output=0, disp=1,\n",
    "              retall=0, callback=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.470132\n",
      "         Iterations: 342\n",
      "         Function evaluations: 2382\n",
      "         Gradient evaluations: 2370\n"
     ]
    }
   ],
   "source": [
    "lbfgs_fitted2 = fmin_bfgs(cost_func, x0=betas, fprime=log_gradient, args=myargs)\n",
    "# , gtol=1e-5, norm=np.Inf,\n",
    "#               epsilon=_epsilon, maxiter=None, full_output=0, disp=1,\n",
    "#               retall=0, callback=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class OptimizeResult(dict):\n",
    "    \"\"\" Represents the optimization result.\n",
    "    Attributes\n",
    "    ----------\n",
    "    x : ndarray\n",
    "        The solution of the optimization.\n",
    "    success : bool\n",
    "        Whether or not the optimizer exited successfully.\n",
    "    status : int\n",
    "        Termination status of the optimizer. Its value depends on the\n",
    "        underlying solver. Refer to `message` for details.\n",
    "    message : str\n",
    "        Description of the cause of the termination.\n",
    "    fun, jac, hess: ndarray\n",
    "        Values of objective function, its Jacobian and its Hessian (if\n",
    "        available). The Hessians may be approximations, see the documentation\n",
    "        of the function in question.\n",
    "    hess_inv : object\n",
    "        Inverse of the objective function's Hessian; may be an approximation.\n",
    "        Not available for all solvers. The type of this attribute may be\n",
    "        either np.ndarray or scipy.sparse.linalg.LinearOperator.\n",
    "    nfev, njev, nhev : int\n",
    "        Number of evaluations of the objective functions and of its\n",
    "        Jacobian and Hessian.\n",
    "    nit : int\n",
    "        Number of iterations performed by the optimizer.\n",
    "    maxcv : float\n",
    "        The maximum constraint violation.\n",
    "    Notes\n",
    "    -----\n",
    "    There may be additional attributes not listed above depending of the\n",
    "    specific solver. Since this class is essentially a subclass of dict\n",
    "    with attribute accessors, one can see which attributes are available\n",
    "    using the `keys()` method.\n",
    "    \"\"\"\n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return self[name]\n",
    "        except KeyError:\n",
    "            raise AttributeError(name)\n",
    "\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.keys():\n",
    "            m = max(map(len, list(self.keys()))) + 1\n",
    "            return '\\n'.join([k.rjust(m) + ': ' + repr(v)\n",
    "                              for k, v in sorted(self.items())])\n",
    "        else:\n",
    "            return self.__class__.__name__ + \"()\"\n",
    "\n",
    "    def __dir__(self):\n",
    "        return list(self.keys())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kpkn",
   "language": "python",
   "name": "kpkn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
